## Blog

experiments to understand LLMs/VLMs and how we may tame them. 

### Oct 25
- [do visual cues help vlms](visual_cues.md): VLMs are very good at identifying events in videos. But it is hard to focus their attention on a specific action or object, specially if the object or action is a small part of the video. Humans use visual cues to attract attention towards a region or action, e.g. a stop sign on the road or a wiggly line drawn under wrongly spelled text. Once focused we can observe finegrained details from that region. This experiment is to determine if using visual cues like bboxes/trajectories will help VLMs focus better.

### May 25

- [models of small worlds](small_worlds.md) : LLMs are terrible at visual reasoning and cannot even reason around basic visual puzzles. 
There is a [million dollar prize](https://www.arcprize.org) to anyone who can make them 85% accurate on visual puzzles that are very simple for humans.
These puzzles use simple concepts such as copying, moving, colliding, large eats small etc. in a small world to create scenarios. 
To solve this, we need to find the precise program which will generate a scenario and apply it to a another instance of that world to solve a puzzle. 
This blog has my ideas, and attempts to solve this problem. 


- [state of video understanding and analytics](video.md) : 


- [agent see agent do](agents.md): how to build agents that learn by looking at you work 


### April 25
- [new internet](internet.md): how internet _might_ look in a few years.  